\section{Experimental Setup and Results}

\subsection{Feature Computation}

We extracted a topic model using 30-topic LDA for all 3,296,605 users active between March 1, 2014 and March 5, 2014, and went on to compute their \texttt{RetweetRank} using all tweets and retweets posted during this period.  

From this period, a random sample of 10,000 tweets were selected, on which \texttt{AlphaPhi} was computed using the twitter graph inferred during this time and all 93,151 users inferred to have seen these 10,000 tweets.  
We also, for a period of 1 hour after the creation of each tweet, record $R_s^{(p)}$ every twenty seconds.
Finally, for the first ten minutes after each tweet is posted, we compute $\alpha_s^{(p)}$, $\phi_s^{(p)}$ and $RR_s^{(p)}$ in twenty second intervals, using the $\phi$'s computed before.


\subsection{The harder problem: Predicting the number of retweets after an hour from data at time $p$.}

It is a very difficult problem to predict very heavy-tailed distributions where most tweets have very few retweets, but some small number have very many.
We attempt it anyways. 
For each minute $p$ in the first ten minutes, we build a machine learning model to predict $\log(R_s^{1hr}+1)$ from the predictors available at time $p$.  
We report the 5 fold cross validation with two repeats for each model and report the root mean squared error in the predicted logarithm.  
The machine learning models we use are linear regression, support vector regressions and random forests (all untuned because of the long time it takes to fit the number of models we want to fit).  

In each run we use a different feature set described in this paper, plus ambient features logs of the friend count and followers count.  

\subsection{The Doable Problem: Predicting Cascades}

As described in \cite{DBLP:journals/corr/ChengADKL14}, there is a brilliant way of overcoming the difficulty of heavy tails.
Balanced classification problems are inherently easier than highly unbalanced regressions.
Luckily, because the number of retweets a tweet eventually gets ought to follow a power law, the probability of a tweet doubling its retweet count is about 50-50.  
Thus, for each $i\in\{1,...,8\}$ we predict, using a support vector machine, if a tweet with at least $2^i$ retweets will reach $2^{i+1}$ retweets.

The features we use for this prediction are the ambient features from before, plus
\[p_{s,i}=min p~:~R_s^{(p)}\ge 2^i\]
and features $RT^{(p_{s,i})}_s$, $\alpha^{(p_{s,i})}_s$ and $\phi^{(p_{s,i})}_s$.
We also experiment with using $\alpha_s$ itself for prediction, although this is impossible in real applications due to it not yet being known.  


\section{Results}

\subsection{Results for the Regression Problem}

The results for regression are summarized in Figure~\ref{fig:linearattempts}, Figure~\ref{fig:svmattempts}, Figure~\ref{fig:rfattempts}.  
In each graph we include the constant predictor (predict using only things known at tweet creation: followers and friends count), and in almost ever case, we beat it.

Without fancy features, the naive first try would be to just use logs of $R_s^{(p)}$: this is the thing to beat.
We look first at the linear model.  
Notice first of all that most features sets which do not use $R_s^{(p)}$ do much worse; the exception being linear models on the partial \texttt{RetweetRank} (Figure~\ref{fig:linearattempts} orange).  
This is sad, but not surprising, as these things try to encode fancy information not directly related to what we are computing, while the retwet rates are literally early predictors of this response, and at least they beat the constant estimator.  
The remaining linear models on features sets including the retweet rates do almost exactly as well as just the retweet rates, as the three nearly identical curves in the middle of Figure~\ref{fig:linearattempts} shows.  


On the SVM, the best feature set is $\alpha_s^{(p)}\log(F_u+1)$ with the retweet rates (Figure~\ref{fig:svmattempts} black).  
It is not surprising a nonlinear fit is required to make this feature set really shine; the partial alphas can go up and down for all sorts of complicated reasons, but apparently the SVM could sort it out.  
The partial alphas by themselves also did well (purple).  The other feature sets did worse than the most naive model (red), although still better than they did with the linear model.  

On Random Forests, we beat the naive when we either add the partial alphas or the partial retweet ranks, and both do about as well as the other.  The phi averages do no worse than the naive, while the features which do not use the retweet rates do worse than the naive approach, but still better than they did in the linear models.  

\subsection{Results for the Cascade Prediction Problem}

The results for this approach, summarized in \ref{fig:jurerocks} are far more positive.  
The baseline approach, the approach-to-beat, is only using the rewtweet rates.  
Excitingly, adding partial alphas (red and blue) beat the baseline!   Knowing the total $\alpha_s$ (red) helps a bit early on, although it seems to have diminishing returns for larger tweets.  
Sadly, the other two features both performed no better than baseline.

\section{Difficulties and Future Work}

One major difficulty we had was the massive pipeline required to compute the features we cared about.
In particular, computing the LDA model and tweet topic took days of compute time and required careful use of AWS resources.
Many things about AWS were found to be non-intuitive, like \texttt{HIVE} require explicit copying of the data from \texttt{S3} to run in any reasonable amount of time and seeming to be very sensitive to the way queries were written.
The long running jobs all dependent on each other were insanity inducing, and we barely finished on time.

Another difficulty is the incomplete data.
While the retweet rates were known almost exactly (since we could read the number of retweets from the JSON file), the other features were build on looking at the individual rewteeters, of whom we knew only 10\%.  This made those features far less robust than the retweet rate itself.
It would be interesting to see if using the entire firehose would improve performance.
Estimates of the graph were likewise annoying, and we wonder if we could build better \texttt{AlphaPhi} estimate if we knew the exact graph.

It would also be interesting to try playing with the number of topics, combining classes of features and tuning machine learning algorithms for better performance.  


\section{Conclusions}

We created two new sets of measures, one of user retweetability and one of tweet quality, and used them to engineer features to predict the number of retweets.
Our initial results with \texttt{AlphaPhi} were promising, we were able to improve both regressions and cascade predictions, although they require a nonlinear algorithm to unlock their full potential.  
The \texttt{TwitterRanke} was very predictive with linear predictors, not requiring non linearity at all.  
We were able to do even better when trying to predict cascades.  
Predicting tweet popularity is a difficult problem, and I think we gave it a good fight.  
